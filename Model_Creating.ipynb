{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:22:18.897116Z",
     "start_time": "2020-03-04T17:22:17.501765Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:22:18.935980Z",
     "start_time": "2020-03-04T17:22:18.921023Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare target\n",
    "\n",
    "def Dummies_Corrected(Data,Test):\n",
    "    train_objs_num = len(Data)\n",
    "    combined = pd.concat([Data, Test], axis=0)\n",
    "    df = pd.get_dummies(combined)\n",
    "    Data[:] = df[:train_objs_num]\n",
    "    Test[:] = df[train_objs_num:]\n",
    "    print(Data)\n",
    "\n",
    "def Categorize(Data):\n",
    "    for x in Data.columns:\n",
    "        if(Data[x].dtype == object):\n",
    "            Data[x] = pd.Categorical(\n",
    "        Data[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:24:14.343074Z",
     "start_time": "2020-03-04T17:24:14.335129Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_outliers(Data, col):\n",
    "    data_mean, data_std = np.mean(Data[col]), np.std(Data[col])\n",
    "    cut_off = data_std * 4\n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "    return Data.loc[(Data[col]<lower) | (Data[col]>upper), col ] #.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:34:19.576137Z",
     "start_time": "2020-03-04T17:34:19.571151Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:23:25.082191Z",
     "start_time": "2020-03-04T17:23:25.050304Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('TrainingDataFeaturescd.csv', \n",
    "                 index_col = 0)\n",
    "X_copy = X_train.copy()\n",
    "\n",
    "# X_test = pd.read_csv('TrainingDataFeatures.csv', \n",
    "#                  index_col = 0)\n",
    "# X_test_copy = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:23:25.538968Z",
     "start_time": "2020-03-04T17:23:25.531954Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_train = X_train['Total52'].copy()\n",
    "# Y_test = X_test['Total52'].copy()\n",
    "\n",
    "X_train.drop('Total52', axis = 1 ,inplace = True)\n",
    "# X_test.drop('Total52', axis = 1 ,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:23:25.993693Z",
     "start_time": "2020-03-04T17:23:25.958786Z"
    }
   },
   "outputs": [],
   "source": [
    "Categorize(X_train)\n",
    "# Categorize(X_test)\n",
    "# Dummies_Corrected(X_train,X_test)\n",
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:23:26.552236Z",
     "start_time": "2020-03-04T17:23:26.508319Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test =train_test_split(X_train,Y_train,\n",
    "                                                   test_size=0.33,\n",
    "                                                   random_state = SEED,\n",
    "                                                   stratify=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:23:30.177294Z",
     "start_time": "2020-03-04T17:23:30.173307Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# x_train, y_train = shuffle(X_train, Y_train, random_state = SEED)\n",
    "# x_test , y_test = X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:26:52.175058Z",
     "start_time": "2020-03-04T17:26:52.120174Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, mutual_info_classif, mutual_info_regression, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def select_mutual(X_train, y_train, X_test, k=10):\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "    X_train = fs.fit_transform(X_train, y_train)\n",
    "    X_test = fs.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def select_forest(X_train, y_train, X_test, k=10):\n",
    "    clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    sfm = SelectFromModel(clf, threshold=-np.inf,max_features=k)\n",
    "    X_train=sfm.fit_transform(X_train, y_train)\n",
    "    X_test = sfm.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def select_chi2(X_train, y_train, X_test, k=10):\n",
    "    fs = SelectKBest(score_func=chi2, k=k)\n",
    "    X_train = fs.fit_transform(X_train, y_train)\n",
    "    X_test = fs.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONS OF RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:24:55.852688Z",
     "start_time": "2020-03-04T17:24:55.833736Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-3a33fc74edcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Selecting features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mx_train_fs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_fs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# # PCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-0ea8513b8e79>\u001b[0m in \u001b[0;36mselect_forest\u001b[1;34m(X_train, y_train, X_test, k)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msfm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# OPTIONS OF RUNNING\n",
    "\n",
    "# Normal\n",
    "\n",
    "# **********\n",
    "\n",
    "# WITHOUT OUTLIERS\n",
    "\n",
    "#X_test = X_test_copy.copy()\n",
    "# X_train = X_copy.copy()\n",
    "# X_train = X_train.drop( find_outliers(X_train,'algo').index )\n",
    "# Y_train = X_train['Total52'].copy()\n",
    "\n",
    "# x_train, x_test, y_train, y_test =train_test_split(X_train,Y_train,\n",
    "#                                                    test_size=0.33,\n",
    "#                                                    random_state = SEED,\n",
    "#                                                    stratify=Y_train)\n",
    "\n",
    "# Non linear?\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)\n",
    "# x_train = poly.fit_transform(x_train)\n",
    "# x_test = poly.transform(x_test)\n",
    "\n",
    "# # PCA \n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(.99)\n",
    "# x_train = pca.fit_transform(x_train)\n",
    "# x_test = pca.transform(x_test);\n",
    "\n",
    "# Hashing\n",
    "\n",
    "# from sklearn.feature_extraction import FeatureHasher\n",
    "# # si input_type = 'dict' Either “dict” (the default) to accept dictionaries over (feature_name, value);\n",
    "# h = FeatureHasher(n_features=5, input_type='string')\n",
    "# f = h.transform(new_class)\n",
    "# f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:27:17.759338Z",
     "start_time": "2020-03-04T17:26:58.468026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting features\n",
    "x_train, x_test = select_forest(x_train,y_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:27:25.031980Z",
     "start_time": "2020-03-04T17:27:24.990235Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def BasedLine2(X_train, y_train,models):\n",
    "    # Test options and evaluation metric\n",
    "    num_folds = 5\n",
    "    \n",
    "\n",
    "    results = []\n",
    "    names = []\n",
    "    for name, model in models:\n",
    "        kfold = StratifiedKFold(n_splits=num_folds, random_state=SEED, shuffle = True)\n",
    "      # classification scoring = ['f1_micro','accuracy','average_precision_score', 'recall']\n",
    "        scoring = ['explained_variance','r2','max_error','neg_root_mean_squared_error']\n",
    "        \n",
    "        cv_results = cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring,\n",
    "                                  return_train_score = False)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "    #    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  #      print(msg)\n",
    "        #print(np.mean(cv_results) )\n",
    "        \n",
    "    return names, results\n",
    "\n",
    "def ScoreDataFrame(names,results):\n",
    "\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    scores = []\n",
    "    for r in range(len(results)):\n",
    "        scores.append(pd.DataFrame(results[r]).mean())\n",
    "    scoreDataFrame = pd.DataFrame(scores,index = names)\n",
    "    return scoreDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T18:12:45.102068Z",
     "start_time": "2020-03-04T18:12:45.079130Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from math import ceil\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "alpha = 2\n",
    "if(Y_train.ndim>1):\n",
    "    Nh = ceil( X_train.shape[0]/(alpha * (X_train.shape[1]+Y_train.shape[1])) )\n",
    "else:\n",
    "    Nh = ceil( X_train.shape[0]/(alpha * (X_train.shape[1])) )\n",
    "\n",
    "max_iter=250\n",
    "\n",
    "def GetScaledModel(nameOfScaler):\n",
    "    \n",
    "    if nameOfScaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif nameOfScaler =='minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    pipelines = []\n",
    "    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression(max_iter=max_iter))])))\n",
    "    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n",
    "    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n",
    "    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n",
    "    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n",
    "    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC(max_iter=max_iter))])))\n",
    "    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n",
    " #   pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n",
    "    \n",
    "    pipelines.append((nameOfScaler+'MLP'  , Pipeline([('Scaler', scaler),\n",
    "                                                      ('MLP'  , MLPClassifier(\n",
    "                                                          hidden_layer_sizes = Nh,\n",
    "                                                          activation= 'relu',\n",
    "                                                          solver = 'adam',\n",
    "                                                          early_stopping=True,\n",
    "                                                          max_iter=max_iter,\n",
    "                                                          ))])  ))\n",
    "\n",
    "    return pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T19:46:47.221239Z",
     "start_time": "2020-03-04T19:46:47.198295Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from math import ceil\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "alpha = 2\n",
    "if(Y_train.ndim>1):\n",
    "    Nh = ceil( X_train.shape[0]/(alpha * (X_train.shape[1]+Y_train.shape[1])) )\n",
    "else:\n",
    "    Nh = ceil( X_train.shape[0]/(alpha * (X_train.shape[1])) )\n",
    "\n",
    "max_iter=250\n",
    "\n",
    "def GetScaledModel_Regression(nameOfScaler):\n",
    "    \n",
    "    if nameOfScaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif nameOfScaler =='minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    pipelines = []\n",
    "    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression(max_iter=max_iter))])))\n",
    "    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n",
    "    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsRegressor())])))\n",
    "    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeRegressor())])))\n",
    "    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n",
    "    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC(max_iter=max_iter))])))\n",
    "    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostRegressor())])  ))\n",
    " #   pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingRegressor())])  ))\n",
    "    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestRegressor())])  ))\n",
    "    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesRegressor())])  ))\n",
    "    \n",
    "    pipelines.append((nameOfScaler+'MLP'  , Pipeline([('Scaler', scaler),\n",
    "                                                      ('MLP'  , MLPRegressor(\n",
    "                                                          hidden_layer_sizes = Nh,\n",
    "                                                          activation= 'relu',\n",
    "                                                          solver = 'adam',\n",
    "                                                          early_stopping=True,\n",
    "                                                          max_iter=max_iter,\n",
    "                                                          ))])  ))\n",
    "\n",
    "    return pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T19:47:34.240953Z",
     "start_time": "2020-03-04T19:46:50.148923Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=250).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=250).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=250).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=250).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=250).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "models = GetScaledModel('standard')\n",
    "names,results = BasedLine2(x_train, y_train,models)\n",
    "scaledScoreStandard = ScoreDataFrame(names,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T19:48:09.469971Z",
     "start_time": "2020-03-04T19:48:09.453016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_explained_variance</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>test_max_error</th>\n",
       "      <th>test_neg_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>standardLR</th>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-40.80</td>\n",
       "      <td>-7.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardLDA</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-41.40</td>\n",
       "      <td>-7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardKNN</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-40.80</td>\n",
       "      <td>-7.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardCART</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-41.00</td>\n",
       "      <td>-8.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardNB</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-2.80</td>\n",
       "      <td>-5.83</td>\n",
       "      <td>-41.00</td>\n",
       "      <td>-21.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardSVM</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-40.80</td>\n",
       "      <td>-7.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardAB</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-42.20</td>\n",
       "      <td>-11.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardRF</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-41.00</td>\n",
       "      <td>-8.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardET</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-40.60</td>\n",
       "      <td>-8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardMLP</th>\n",
       "      <td>2.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-40.80</td>\n",
       "      <td>-7.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              fit_time  score_time  test_explained_variance  test_r2  \\\n",
       "standardLR        1.85        0.00                     0.28     0.17   \n",
       "standardLDA       0.02        0.00                     0.21     0.19   \n",
       "standardKNN       0.03        0.07                     0.28     0.18   \n",
       "standardCART      0.02        0.00                     0.03     0.02   \n",
       "standardNB        0.01        0.01                    -2.80    -5.83   \n",
       "standardSVM       0.90        0.32                     0.26     0.13   \n",
       "standardAB        0.62        0.05                    -0.83    -0.94   \n",
       "standardRF        0.69        0.06                     0.03     0.02   \n",
       "standardET        1.44        0.07                     0.06     0.04   \n",
       "standardMLP       2.60        0.01                     0.29     0.17   \n",
       "\n",
       "              test_max_error  test_neg_root_mean_squared_error  \n",
       "standardLR            -40.80                             -7.44  \n",
       "standardLDA           -41.40                             -7.35  \n",
       "standardKNN           -40.80                             -7.41  \n",
       "standardCART          -41.00                             -8.08  \n",
       "standardNB            -41.00                            -21.32  \n",
       "standardSVM           -40.80                             -7.61  \n",
       "standardAB            -42.20                            -11.15  \n",
       "standardRF            -41.00                             -8.09  \n",
       "standardET            -40.60                             -8.00  \n",
       "standardMLP           -40.80                             -7.45  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledScoreStandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:49:57.798022Z",
     "start_time": "2020-03-04T17:49:57.794035Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "# classification score = ['f1_micro','accuracy','average_precision_score', 'recall']\n",
    "# score = ['explained_variance','r2','max_error','neg_root_mean_squared_error']\n",
    "score = 'r2'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:00:43.614838Z",
     "start_time": "2020-03-04T20:00:43.537713Z"
    }
   },
   "outputs": [],
   "source": [
    "# First Scalling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "x_train = stdsc.fit_transform(x_train)\n",
    "x_test = stdsc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T18:01:48.041347Z",
     "start_time": "2020-03-04T18:01:48.030377Z"
    }
   },
   "outputs": [],
   "source": [
    "# rf_param_grid = {\n",
    "#                  'max_depth' : [3,20,50],\n",
    "#                  'n_estimators': [100,1000,2000],\n",
    "#                  'max_features': ['sqrt','auto','log2'],\n",
    "#                  'min_samples_split': [2,6,8],\n",
    "#                  'min_samples_leaf': [2,6,8],\n",
    "#                  'bootstrap': [True, False]\n",
    "#                  }\n",
    "\n",
    "rf_param_grid = {\n",
    "    # randomly sample numbers from 4 to 204 estimators\n",
    "    'n_estimators': randint(4,200),\n",
    "    # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n",
    "    'max_features': truncnorm(a=0, b=1, loc=0.25, scale=0.1), # 0 to 25% obs\n",
    "    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n",
    "    'min_samples_split': uniform(0.00and1, 0.199) # 0 to 20% obs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T18:03:58.012193Z",
     "start_time": "2020-03-04T18:01:54.184852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 0.32050095083279484, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 0.0114215117805207, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 168, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "41           0.50          0.02             0.07            0.00   \n",
      "79           0.06          0.00             0.01            0.00   \n",
      "55           0.42          0.03             0.06            0.00   \n",
      "77           0.29          0.00             0.04            0.00   \n",
      "3            0.19          0.00             0.03            0.00   \n",
      "7            0.60          0.06             0.08            0.01   \n",
      "90           0.06          0.00             0.01            0.00   \n",
      "38           0.03          0.00             0.00            0.00   \n",
      "76           0.48          0.01             0.06            0.00   \n",
      "86           0.40          0.02             0.06            0.00   \n",
      "28           0.29          0.01             0.05            0.00   \n",
      "62           0.31          0.01             0.05            0.01   \n",
      "9            0.47          0.02             0.07            0.00   \n",
      "27           0.21          0.01             0.04            0.01   \n",
      "44           0.10          0.01             0.02            0.00   \n",
      "92           0.04          0.00             0.01            0.00   \n",
      "69           0.31          0.02             0.04            0.00   \n",
      "18           0.33          0.02             0.04            0.00   \n",
      "8            0.09          0.00             0.01            0.00   \n",
      "29           0.31          0.01             0.05            0.01   \n",
      "52           0.42          0.01             0.06            0.00   \n",
      "75           0.30          0.01             0.05            0.00   \n",
      "71           0.48          0.06             0.07            0.01   \n",
      "73           0.43          0.02             0.06            0.00   \n",
      "6            0.13          0.01             0.02            0.00   \n",
      "5            0.05          0.00             0.01            0.00   \n",
      "1            0.39          0.02             0.06            0.00   \n",
      "54           0.11          0.01             0.02            0.00   \n",
      "45           0.09          0.00             0.02            0.00   \n",
      "66           0.18          0.01             0.03            0.00   \n",
      "..            ...           ...              ...             ...   \n",
      "24           0.08          0.00             0.01            0.00   \n",
      "10           0.36          0.02             0.06            0.00   \n",
      "78           0.16          0.01             0.03            0.00   \n",
      "47           0.16          0.00             0.03            0.00   \n",
      "34           0.04          0.00             0.01            0.00   \n",
      "68           0.14          0.00             0.02            0.00   \n",
      "93           0.17          0.00             0.03            0.00   \n",
      "21           0.20          0.00             0.03            0.00   \n",
      "40           0.34          0.01             0.06            0.00   \n",
      "26           0.08          0.01             0.01            0.00   \n",
      "37           0.02          0.00             0.00            0.00   \n",
      "0            0.13          0.02             0.02            0.00   \n",
      "97           0.18          0.02             0.04            0.01   \n",
      "64           0.19          0.00             0.03            0.00   \n",
      "61           0.20          0.02             0.04            0.00   \n",
      "33           0.19          0.01             0.03            0.00   \n",
      "83           0.29          0.02             0.05            0.00   \n",
      "50           0.25          0.00             0.05            0.01   \n",
      "88           0.08          0.00             0.02            0.00   \n",
      "91           0.18          0.01             0.03            0.00   \n",
      "94           0.08          0.01             0.01            0.00   \n",
      "95           0.16          0.00             0.03            0.00   \n",
      "74           0.04          0.00             0.01            0.00   \n",
      "42           0.25          0.01             0.05            0.00   \n",
      "32           0.03          0.00             0.01            0.00   \n",
      "65           0.09          0.00             0.02            0.00   \n",
      "99           0.24          0.01             0.04            0.00   \n",
      "25           0.08          0.00             0.01            0.00   \n",
      "80           0.17          0.01             0.03            0.00   \n",
      "60           0.27          0.00             0.05            0.00   \n",
      "\n",
      "   param_max_features param_min_samples_split param_n_estimators  \\\n",
      "41               0.32                    0.01                168   \n",
      "79               0.28                    0.01                 20   \n",
      "55               0.25                    0.01                147   \n",
      "77               0.27                    0.01                106   \n",
      "3                0.29                    0.01                 72   \n",
      "7                0.34                    0.01                193   \n",
      "90               0.28                    0.02                 22   \n",
      "38               0.31                    0.02                 10   \n",
      "76               0.32                    0.02                174   \n",
      "86               0.29                    0.03                175   \n",
      "28               0.25                    0.03                121   \n",
      "62               0.27                    0.03                139   \n",
      "9                0.34                    0.04                199   \n",
      "27               0.33                    0.04                 83   \n",
      "44               0.34                    0.04                 36   \n",
      "92               0.31                    0.03                 16   \n",
      "69               0.32                    0.04                128   \n",
      "18               0.34                    0.05                131   \n",
      "8                0.34                    0.06                 42   \n",
      "29               0.29                    0.04                139   \n",
      "52               0.33                    0.04                184   \n",
      "75               0.25                    0.05                137   \n",
      "71               0.35                    0.05                185   \n",
      "73               0.33                    0.06                188   \n",
      "6                0.31                    0.08                 60   \n",
      "5                0.26                    0.07                 23   \n",
      "1                0.35                    0.10                189   \n",
      "54               0.29                    0.09                 58   \n",
      "45               0.30                    0.10                 49   \n",
      "66               0.27                    0.07                 95   \n",
      "..                ...                     ...                ...   \n",
      "24               0.26                    0.14                 47   \n",
      "10               0.32                    0.14                196   \n",
      "78               0.27                    0.14                 92   \n",
      "47               0.25                    0.15                 99   \n",
      "34               0.34                    0.14                 21   \n",
      "68               0.27                    0.14                 80   \n",
      "93               0.26                    0.14                100   \n",
      "21               0.29                    0.14                111   \n",
      "40               0.30                    0.15                187   \n",
      "26               0.29                    0.14                 43   \n",
      "37               0.28                    0.18                  8   \n",
      "0                0.26                    0.17                 71   \n",
      "97               0.25                    0.16                 88   \n",
      "64               0.34                    0.18                109   \n",
      "61               0.25                    0.18                114   \n",
      "33               0.26                    0.17                110   \n",
      "83               0.27                    0.17                164   \n",
      "50               0.26                    0.18                157   \n",
      "88               0.29                    0.16                 49   \n",
      "91               0.27                    0.18                107   \n",
      "94               0.27                    0.19                 41   \n",
      "95               0.29                    0.19                 95   \n",
      "74               0.29                    0.19                 22   \n",
      "42               0.28                    0.20                147   \n",
      "32               0.30                    0.18                 16   \n",
      "65               0.26                    0.19                 53   \n",
      "99               0.28                    0.21                154   \n",
      "25               0.27                    0.20                 47   \n",
      "80               0.35                    0.20                 97   \n",
      "60               0.25                    0.20                160   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "41  {'max_features': 0.32050095083279484, 'min_sam...               0.14   \n",
      "79  {'max_features': 0.2837380993015824, 'min_samp...               0.17   \n",
      "55  {'max_features': 0.2538312475450465, 'min_samp...               0.13   \n",
      "77  {'max_features': 0.27087258777091305, 'min_sam...               0.13   \n",
      "3   {'max_features': 0.28668927780580344, 'min_sam...               0.14   \n",
      "7   {'max_features': 0.34072484358174615, 'min_sam...               0.13   \n",
      "90  {'max_features': 0.2772896908183179, 'min_samp...               0.13   \n",
      "38  {'max_features': 0.3097341375344037, 'min_samp...               0.12   \n",
      "76  {'max_features': 0.3195648148304188, 'min_samp...               0.12   \n",
      "86  {'max_features': 0.28902776172562983, 'min_sam...               0.11   \n",
      "28  {'max_features': 0.2533928672210729, 'min_samp...               0.11   \n",
      "62  {'max_features': 0.2709352477762488, 'min_samp...               0.11   \n",
      "9   {'max_features': 0.3379098978706627, 'min_samp...               0.11   \n",
      "27  {'max_features': 0.3296859406324454, 'min_samp...               0.11   \n",
      "44  {'max_features': 0.3421026275418799, 'min_samp...               0.10   \n",
      "92  {'max_features': 0.311898056658579, 'min_sampl...               0.11   \n",
      "69  {'max_features': 0.31795369133546963, 'min_sam...               0.11   \n",
      "18  {'max_features': 0.33756977686546197, 'min_sam...               0.11   \n",
      "8   {'max_features': 0.3431961207073071, 'min_samp...               0.11   \n",
      "29  {'max_features': 0.2928438058166438, 'min_samp...               0.10   \n",
      "52  {'max_features': 0.330141446660537, 'min_sampl...               0.10   \n",
      "75  {'max_features': 0.25015868827926563, 'min_sam...               0.10   \n",
      "71  {'max_features': 0.3469503037804131, 'min_samp...               0.10   \n",
      "73  {'max_features': 0.327995099627898, 'min_sampl...               0.10   \n",
      "6   {'max_features': 0.3133829749087653, 'min_samp...               0.10   \n",
      "5   {'max_features': 0.25564467077774433, 'min_sam...               0.10   \n",
      "1   {'max_features': 0.3469737191337041, 'min_samp...               0.09   \n",
      "54  {'max_features': 0.29070217042516266, 'min_sam...               0.09   \n",
      "45  {'max_features': 0.3019429051381534, 'min_samp...               0.08   \n",
      "66  {'max_features': 0.26840050097398654, 'min_sam...               0.10   \n",
      "..                                                ...                ...   \n",
      "24  {'max_features': 0.2631924653799897, 'min_samp...               0.09   \n",
      "10  {'max_features': 0.31939667287499185, 'min_sam...               0.07   \n",
      "78  {'max_features': 0.2748597431680584, 'min_samp...               0.04   \n",
      "47  {'max_features': 0.2516391642123631, 'min_samp...               0.03   \n",
      "34  {'max_features': 0.33519224131235903, 'min_sam...               0.01   \n",
      "68  {'max_features': 0.26706585690456447, 'min_sam...              -0.00   \n",
      "93  {'max_features': 0.25851067852734533, 'min_sam...               0.00   \n",
      "21  {'max_features': 0.28539519937275837, 'min_sam...               0.01   \n",
      "40  {'max_features': 0.30381323520240094, 'min_sam...               0.01   \n",
      "26  {'max_features': 0.2893134141667649, 'min_samp...               0.04   \n",
      "37  {'max_features': 0.2847861387468348, 'min_samp...               0.06   \n",
      "0   {'max_features': 0.2565337691858373, 'min_samp...               0.05   \n",
      "97  {'max_features': 0.253018531021254, 'min_sampl...              -0.01   \n",
      "64  {'max_features': 0.33835455172992307, 'min_sam...              -0.05   \n",
      "61  {'max_features': 0.2509052550079607, 'min_samp...               0.02   \n",
      "33  {'max_features': 0.2594262824927381, 'min_samp...              -0.03   \n",
      "83  {'max_features': 0.265886711497229, 'min_sampl...              -0.04   \n",
      "50  {'max_features': 0.2645195547920704, 'min_samp...              -0.04   \n",
      "88  {'max_features': 0.2940329381111786, 'min_samp...              -0.02   \n",
      "91  {'max_features': 0.2723092305815467, 'min_samp...              -0.00   \n",
      "94  {'max_features': 0.2734346879311414, 'min_samp...              -0.07   \n",
      "95  {'max_features': 0.290211138184583, 'min_sampl...              -0.11   \n",
      "74  {'max_features': 0.29434756772212495, 'min_sam...              -0.05   \n",
      "42  {'max_features': 0.28419551357813755, 'min_sam...              -0.11   \n",
      "32  {'max_features': 0.29606047047625894, 'min_sam...              -0.07   \n",
      "65  {'max_features': 0.2603400004237625, 'min_samp...              -0.12   \n",
      "99  {'max_features': 0.2792492309654589, 'min_samp...              -0.11   \n",
      "25  {'max_features': 0.27025512862349726, 'min_sam...              -0.14   \n",
      "80  {'max_features': 0.3463257215073301, 'min_samp...              -0.11   \n",
      "60  {'max_features': 0.2533356223702762, 'min_samp...              -0.11   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "41               0.19               0.16               0.13   \n",
      "79               0.17               0.13               0.15   \n",
      "55               0.18               0.14               0.14   \n",
      "77               0.18               0.15               0.13   \n",
      "3                0.16               0.13               0.13   \n",
      "7                0.15               0.12               0.14   \n",
      "90               0.15               0.12               0.12   \n",
      "38               0.09               0.08               0.14   \n",
      "76               0.11               0.10               0.11   \n",
      "86               0.09               0.09               0.11   \n",
      "28               0.08               0.09               0.10   \n",
      "62               0.09               0.10               0.10   \n",
      "9                0.09               0.09               0.10   \n",
      "27               0.09               0.09               0.10   \n",
      "44               0.07               0.10               0.10   \n",
      "92               0.09               0.08               0.11   \n",
      "69               0.08               0.08               0.09   \n",
      "18               0.09               0.09               0.09   \n",
      "8                0.10               0.10               0.08   \n",
      "29               0.08               0.09               0.09   \n",
      "52               0.09               0.09               0.08   \n",
      "75               0.08               0.08               0.10   \n",
      "71               0.08               0.08               0.08   \n",
      "73               0.09               0.08               0.08   \n",
      "6                0.09               0.07               0.08   \n",
      "5                0.06               0.07               0.07   \n",
      "1                0.07               0.07               0.07   \n",
      "54               0.07               0.08               0.08   \n",
      "45               0.06               0.08               0.09   \n",
      "66               0.08               0.07               0.06   \n",
      "..                ...                ...                ...   \n",
      "24              -0.01               0.01              -0.01   \n",
      "10              -0.02              -0.01               0.02   \n",
      "78              -0.03              -0.02              -0.01   \n",
      "47              -0.03              -0.01              -0.00   \n",
      "34              -0.02              -0.01              -0.01   \n",
      "68              -0.02              -0.02              -0.00   \n",
      "93              -0.02              -0.02              -0.03   \n",
      "21              -0.03              -0.02              -0.02   \n",
      "40              -0.03              -0.03              -0.02   \n",
      "26              -0.03              -0.03              -0.02   \n",
      "37              -0.01              -0.01              -0.12   \n",
      "0               -0.05              -0.04              -0.05   \n",
      "97              -0.04              -0.05              -0.01   \n",
      "64              -0.03              -0.03              -0.02   \n",
      "61              -0.07              -0.04              -0.03   \n",
      "33              -0.04              -0.04              -0.02   \n",
      "83              -0.04              -0.04              -0.04   \n",
      "50              -0.04              -0.04              -0.04   \n",
      "88              -0.04              -0.08              -0.01   \n",
      "91              -0.08              -0.06              -0.04   \n",
      "94              -0.07              -0.06              -0.04   \n",
      "95              -0.10              -0.05              -0.04   \n",
      "74              -0.13              -0.08              -0.11   \n",
      "42              -0.10              -0.13              -0.08   \n",
      "32              -0.13              -0.09              -0.12   \n",
      "65              -0.14              -0.09              -0.13   \n",
      "99              -0.14              -0.08              -0.13   \n",
      "25              -0.13              -0.14              -0.07   \n",
      "80              -0.13              -0.12              -0.12   \n",
      "60              -0.13              -0.13              -0.12   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "41               0.17             0.16            0.02                1  \n",
      "79               0.17             0.16            0.02                2  \n",
      "55               0.17             0.15            0.02                3  \n",
      "77               0.15             0.15            0.02                4  \n",
      "3                0.16             0.14            0.01                5  \n",
      "7                0.15             0.14            0.01                6  \n",
      "90               0.12             0.13            0.01                7  \n",
      "38               0.15             0.12            0.03                8  \n",
      "76               0.11             0.11            0.00                9  \n",
      "86               0.11             0.10            0.01               10  \n",
      "28               0.12             0.10            0.01               11  \n",
      "62               0.11             0.10            0.01               12  \n",
      "9                0.11             0.10            0.01               13  \n",
      "27               0.10             0.10            0.01               14  \n",
      "44               0.11             0.09            0.01               15  \n",
      "92               0.08             0.09            0.01               16  \n",
      "69               0.11             0.09            0.01               17  \n",
      "18               0.09             0.09            0.01               18  \n",
      "8                0.08             0.09            0.01               19  \n",
      "29               0.10             0.09            0.01               20  \n",
      "52               0.09             0.09            0.01               21  \n",
      "75               0.09             0.09            0.01               22  \n",
      "71               0.09             0.09            0.01               23  \n",
      "73               0.09             0.09            0.01               24  \n",
      "6                0.08             0.08            0.01               25  \n",
      "5                0.10             0.08            0.01               26  \n",
      "1                0.09             0.08            0.01               27  \n",
      "54               0.07             0.08            0.01               28  \n",
      "45               0.09             0.08            0.01               29  \n",
      "66               0.07             0.08            0.01               30  \n",
      "..                ...              ...             ...              ...  \n",
      "24              -0.03             0.01            0.04               71  \n",
      "10              -0.01             0.01            0.03               72  \n",
      "78               0.02             0.00            0.03               73  \n",
      "47              -0.02            -0.01            0.02               74  \n",
      "34              -0.01            -0.01            0.01               75  \n",
      "68              -0.02            -0.01            0.01               76  \n",
      "93              -0.00            -0.01            0.01               77  \n",
      "21              -0.01            -0.01            0.01               78  \n",
      "40              -0.02            -0.02            0.01               79  \n",
      "26              -0.05            -0.02            0.03               80  \n",
      "37              -0.03            -0.02            0.06               81  \n",
      "0               -0.04            -0.03            0.04               82  \n",
      "97              -0.03            -0.03            0.01               83  \n",
      "64              -0.02            -0.03            0.01               84  \n",
      "61              -0.05            -0.03            0.03               85  \n",
      "33              -0.05            -0.04            0.01               86  \n",
      "83              -0.03            -0.04            0.00               87  \n",
      "50              -0.04            -0.04            0.00               88  \n",
      "88              -0.07            -0.04            0.03               89  \n",
      "91              -0.04            -0.04            0.02               90  \n",
      "94              -0.04            -0.06            0.01               91  \n",
      "95              -0.04            -0.07            0.03               92  \n",
      "74              -0.08            -0.09            0.03               93  \n",
      "42              -0.09            -0.10            0.02               94  \n",
      "32              -0.12            -0.10            0.02               95  \n",
      "65              -0.07            -0.11            0.03               96  \n",
      "99              -0.13            -0.12            0.02               97  \n",
      "25              -0.13            -0.12            0.03               98  \n",
      "80              -0.12            -0.12            0.01               99  \n",
      "60              -0.13            -0.12            0.01              100  \n",
      "\n",
      "[100 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "clf = RandomizedSearchCV(rf_model, rf_param_grid, n_iter=100, cv=5, random_state=SEED, scoring = score)\n",
    "model = clf.fit(x_train, y_train)\n",
    "rf_best = model.best_estimator_.get_params()\n",
    "print(pd.DataFrame(rf_best))\n",
    "cv_results = model.cv_results_\n",
    "scores = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T18:04:56.219164Z",
     "start_time": "2020-03-04T18:04:56.145366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>168</td>\n",
       "      <td>{'max_features': 0.32050095083279484, 'min_sam...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_features': 0.2837380993015824, 'min_samp...</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>147</td>\n",
       "      <td>{'max_features': 0.2538312475450465, 'min_samp...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.01</td>\n",
       "      <td>106</td>\n",
       "      <td>{'max_features': 0.27087258777091305, 'min_sam...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>72</td>\n",
       "      <td>{'max_features': 0.28668927780580344, 'min_sam...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>193</td>\n",
       "      <td>{'max_features': 0.34072484358174615, 'min_sam...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_features': 0.2772896908183179, 'min_samp...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_features': 0.3097341375344037, 'min_samp...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.02</td>\n",
       "      <td>174</td>\n",
       "      <td>{'max_features': 0.3195648148304188, 'min_samp...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>175</td>\n",
       "      <td>{'max_features': 0.28902776172562983, 'min_sam...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.03</td>\n",
       "      <td>121</td>\n",
       "      <td>{'max_features': 0.2533928672210729, 'min_samp...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>139</td>\n",
       "      <td>{'max_features': 0.2709352477762488, 'min_samp...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.04</td>\n",
       "      <td>199</td>\n",
       "      <td>{'max_features': 0.3379098978706627, 'min_samp...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>83</td>\n",
       "      <td>{'max_features': 0.3296859406324454, 'min_samp...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.04</td>\n",
       "      <td>36</td>\n",
       "      <td>{'max_features': 0.3421026275418799, 'min_samp...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.03</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_features': 0.311898056658579, 'min_sampl...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.04</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_features': 0.31795369133546963, 'min_sam...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>131</td>\n",
       "      <td>{'max_features': 0.33756977686546197, 'min_sam...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.06</td>\n",
       "      <td>42</td>\n",
       "      <td>{'max_features': 0.3431961207073071, 'min_samp...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>139</td>\n",
       "      <td>{'max_features': 0.2928438058166438, 'min_samp...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>184</td>\n",
       "      <td>{'max_features': 0.330141446660537, 'min_sampl...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>137</td>\n",
       "      <td>{'max_features': 0.25015868827926563, 'min_sam...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>185</td>\n",
       "      <td>{'max_features': 0.3469503037804131, 'min_samp...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>188</td>\n",
       "      <td>{'max_features': 0.327995099627898, 'min_sampl...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.08</td>\n",
       "      <td>60</td>\n",
       "      <td>{'max_features': 0.3133829749087653, 'min_samp...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.07</td>\n",
       "      <td>23</td>\n",
       "      <td>{'max_features': 0.25564467077774433, 'min_sam...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.10</td>\n",
       "      <td>189</td>\n",
       "      <td>{'max_features': 0.3469737191337041, 'min_samp...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>58</td>\n",
       "      <td>{'max_features': 0.29070217042516266, 'min_sam...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>49</td>\n",
       "      <td>{'max_features': 0.3019429051381534, 'min_samp...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>95</td>\n",
       "      <td>{'max_features': 0.26840050097398654, 'min_sam...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>47</td>\n",
       "      <td>{'max_features': 0.2631924653799897, 'min_samp...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>196</td>\n",
       "      <td>{'max_features': 0.31939667287499185, 'min_sam...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "      <td>92</td>\n",
       "      <td>{'max_features': 0.2748597431680584, 'min_samp...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>99</td>\n",
       "      <td>{'max_features': 0.2516391642123631, 'min_samp...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.14</td>\n",
       "      <td>21</td>\n",
       "      <td>{'max_features': 0.33519224131235903, 'min_sam...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "      <td>80</td>\n",
       "      <td>{'max_features': 0.26706585690456447, 'min_sam...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_features': 0.25851067852734533, 'min_sam...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>111</td>\n",
       "      <td>{'max_features': 0.28539519937275837, 'min_sam...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>187</td>\n",
       "      <td>{'max_features': 0.30381323520240094, 'min_sam...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>43</td>\n",
       "      <td>{'max_features': 0.2893134141667649, 'min_samp...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.18</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_features': 0.2847861387468348, 'min_samp...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>71</td>\n",
       "      <td>{'max_features': 0.2565337691858373, 'min_samp...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>88</td>\n",
       "      <td>{'max_features': 0.253018531021254, 'min_sampl...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.18</td>\n",
       "      <td>109</td>\n",
       "      <td>{'max_features': 0.33835455172992307, 'min_sam...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>114</td>\n",
       "      <td>{'max_features': 0.2509052550079607, 'min_samp...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>110</td>\n",
       "      <td>{'max_features': 0.2594262824927381, 'min_samp...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>164</td>\n",
       "      <td>{'max_features': 0.265886711497229, 'min_sampl...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>157</td>\n",
       "      <td>{'max_features': 0.2645195547920704, 'min_samp...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49</td>\n",
       "      <td>{'max_features': 0.2940329381111786, 'min_samp...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>107</td>\n",
       "      <td>{'max_features': 0.2723092305815467, 'min_samp...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.19</td>\n",
       "      <td>41</td>\n",
       "      <td>{'max_features': 0.2734346879311414, 'min_samp...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>95</td>\n",
       "      <td>{'max_features': 0.290211138184583, 'min_sampl...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_features': 0.29434756772212495, 'min_sam...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "      <td>147</td>\n",
       "      <td>{'max_features': 0.28419551357813755, 'min_sam...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_features': 0.29606047047625894, 'min_sam...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.19</td>\n",
       "      <td>53</td>\n",
       "      <td>{'max_features': 0.2603400004237625, 'min_samp...</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>154</td>\n",
       "      <td>{'max_features': 0.2792492309654589, 'min_samp...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>47</td>\n",
       "      <td>{'max_features': 0.27025512862349726, 'min_sam...</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.20</td>\n",
       "      <td>97</td>\n",
       "      <td>{'max_features': 0.3463257215073301, 'min_samp...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>160</td>\n",
       "      <td>{'max_features': 0.2533356223702762, 'min_samp...</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "41           0.50          0.02             0.07            0.00   \n",
       "79           0.06          0.00             0.01            0.00   \n",
       "55           0.42          0.03             0.06            0.00   \n",
       "77           0.29          0.00             0.04            0.00   \n",
       "3            0.19          0.00             0.03            0.00   \n",
       "7            0.60          0.06             0.08            0.01   \n",
       "90           0.06          0.00             0.01            0.00   \n",
       "38           0.03          0.00             0.00            0.00   \n",
       "76           0.48          0.01             0.06            0.00   \n",
       "86           0.40          0.02             0.06            0.00   \n",
       "28           0.29          0.01             0.05            0.00   \n",
       "62           0.31          0.01             0.05            0.01   \n",
       "9            0.47          0.02             0.07            0.00   \n",
       "27           0.21          0.01             0.04            0.01   \n",
       "44           0.10          0.01             0.02            0.00   \n",
       "92           0.04          0.00             0.01            0.00   \n",
       "69           0.31          0.02             0.04            0.00   \n",
       "18           0.33          0.02             0.04            0.00   \n",
       "8            0.09          0.00             0.01            0.00   \n",
       "29           0.31          0.01             0.05            0.01   \n",
       "52           0.42          0.01             0.06            0.00   \n",
       "75           0.30          0.01             0.05            0.00   \n",
       "71           0.48          0.06             0.07            0.01   \n",
       "73           0.43          0.02             0.06            0.00   \n",
       "6            0.13          0.01             0.02            0.00   \n",
       "5            0.05          0.00             0.01            0.00   \n",
       "1            0.39          0.02             0.06            0.00   \n",
       "54           0.11          0.01             0.02            0.00   \n",
       "45           0.09          0.00             0.02            0.00   \n",
       "66           0.18          0.01             0.03            0.00   \n",
       "..            ...           ...              ...             ...   \n",
       "24           0.08          0.00             0.01            0.00   \n",
       "10           0.36          0.02             0.06            0.00   \n",
       "78           0.16          0.01             0.03            0.00   \n",
       "47           0.16          0.00             0.03            0.00   \n",
       "34           0.04          0.00             0.01            0.00   \n",
       "68           0.14          0.00             0.02            0.00   \n",
       "93           0.17          0.00             0.03            0.00   \n",
       "21           0.20          0.00             0.03            0.00   \n",
       "40           0.34          0.01             0.06            0.00   \n",
       "26           0.08          0.01             0.01            0.00   \n",
       "37           0.02          0.00             0.00            0.00   \n",
       "0            0.13          0.02             0.02            0.00   \n",
       "97           0.18          0.02             0.04            0.01   \n",
       "64           0.19          0.00             0.03            0.00   \n",
       "61           0.20          0.02             0.04            0.00   \n",
       "33           0.19          0.01             0.03            0.00   \n",
       "83           0.29          0.02             0.05            0.00   \n",
       "50           0.25          0.00             0.05            0.01   \n",
       "88           0.08          0.00             0.02            0.00   \n",
       "91           0.18          0.01             0.03            0.00   \n",
       "94           0.08          0.01             0.01            0.00   \n",
       "95           0.16          0.00             0.03            0.00   \n",
       "74           0.04          0.00             0.01            0.00   \n",
       "42           0.25          0.01             0.05            0.00   \n",
       "32           0.03          0.00             0.01            0.00   \n",
       "65           0.09          0.00             0.02            0.00   \n",
       "99           0.24          0.01             0.04            0.00   \n",
       "25           0.08          0.00             0.01            0.00   \n",
       "80           0.17          0.01             0.03            0.00   \n",
       "60           0.27          0.00             0.05            0.00   \n",
       "\n",
       "   param_max_features param_min_samples_split param_n_estimators  \\\n",
       "41               0.32                    0.01                168   \n",
       "79               0.28                    0.01                 20   \n",
       "55               0.25                    0.01                147   \n",
       "77               0.27                    0.01                106   \n",
       "3                0.29                    0.01                 72   \n",
       "7                0.34                    0.01                193   \n",
       "90               0.28                    0.02                 22   \n",
       "38               0.31                    0.02                 10   \n",
       "76               0.32                    0.02                174   \n",
       "86               0.29                    0.03                175   \n",
       "28               0.25                    0.03                121   \n",
       "62               0.27                    0.03                139   \n",
       "9                0.34                    0.04                199   \n",
       "27               0.33                    0.04                 83   \n",
       "44               0.34                    0.04                 36   \n",
       "92               0.31                    0.03                 16   \n",
       "69               0.32                    0.04                128   \n",
       "18               0.34                    0.05                131   \n",
       "8                0.34                    0.06                 42   \n",
       "29               0.29                    0.04                139   \n",
       "52               0.33                    0.04                184   \n",
       "75               0.25                    0.05                137   \n",
       "71               0.35                    0.05                185   \n",
       "73               0.33                    0.06                188   \n",
       "6                0.31                    0.08                 60   \n",
       "5                0.26                    0.07                 23   \n",
       "1                0.35                    0.10                189   \n",
       "54               0.29                    0.09                 58   \n",
       "45               0.30                    0.10                 49   \n",
       "66               0.27                    0.07                 95   \n",
       "..                ...                     ...                ...   \n",
       "24               0.26                    0.14                 47   \n",
       "10               0.32                    0.14                196   \n",
       "78               0.27                    0.14                 92   \n",
       "47               0.25                    0.15                 99   \n",
       "34               0.34                    0.14                 21   \n",
       "68               0.27                    0.14                 80   \n",
       "93               0.26                    0.14                100   \n",
       "21               0.29                    0.14                111   \n",
       "40               0.30                    0.15                187   \n",
       "26               0.29                    0.14                 43   \n",
       "37               0.28                    0.18                  8   \n",
       "0                0.26                    0.17                 71   \n",
       "97               0.25                    0.16                 88   \n",
       "64               0.34                    0.18                109   \n",
       "61               0.25                    0.18                114   \n",
       "33               0.26                    0.17                110   \n",
       "83               0.27                    0.17                164   \n",
       "50               0.26                    0.18                157   \n",
       "88               0.29                    0.16                 49   \n",
       "91               0.27                    0.18                107   \n",
       "94               0.27                    0.19                 41   \n",
       "95               0.29                    0.19                 95   \n",
       "74               0.29                    0.19                 22   \n",
       "42               0.28                    0.20                147   \n",
       "32               0.30                    0.18                 16   \n",
       "65               0.26                    0.19                 53   \n",
       "99               0.28                    0.21                154   \n",
       "25               0.27                    0.20                 47   \n",
       "80               0.35                    0.20                 97   \n",
       "60               0.25                    0.20                160   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "41  {'max_features': 0.32050095083279484, 'min_sam...               0.14   \n",
       "79  {'max_features': 0.2837380993015824, 'min_samp...               0.17   \n",
       "55  {'max_features': 0.2538312475450465, 'min_samp...               0.13   \n",
       "77  {'max_features': 0.27087258777091305, 'min_sam...               0.13   \n",
       "3   {'max_features': 0.28668927780580344, 'min_sam...               0.14   \n",
       "7   {'max_features': 0.34072484358174615, 'min_sam...               0.13   \n",
       "90  {'max_features': 0.2772896908183179, 'min_samp...               0.13   \n",
       "38  {'max_features': 0.3097341375344037, 'min_samp...               0.12   \n",
       "76  {'max_features': 0.3195648148304188, 'min_samp...               0.12   \n",
       "86  {'max_features': 0.28902776172562983, 'min_sam...               0.11   \n",
       "28  {'max_features': 0.2533928672210729, 'min_samp...               0.11   \n",
       "62  {'max_features': 0.2709352477762488, 'min_samp...               0.11   \n",
       "9   {'max_features': 0.3379098978706627, 'min_samp...               0.11   \n",
       "27  {'max_features': 0.3296859406324454, 'min_samp...               0.11   \n",
       "44  {'max_features': 0.3421026275418799, 'min_samp...               0.10   \n",
       "92  {'max_features': 0.311898056658579, 'min_sampl...               0.11   \n",
       "69  {'max_features': 0.31795369133546963, 'min_sam...               0.11   \n",
       "18  {'max_features': 0.33756977686546197, 'min_sam...               0.11   \n",
       "8   {'max_features': 0.3431961207073071, 'min_samp...               0.11   \n",
       "29  {'max_features': 0.2928438058166438, 'min_samp...               0.10   \n",
       "52  {'max_features': 0.330141446660537, 'min_sampl...               0.10   \n",
       "75  {'max_features': 0.25015868827926563, 'min_sam...               0.10   \n",
       "71  {'max_features': 0.3469503037804131, 'min_samp...               0.10   \n",
       "73  {'max_features': 0.327995099627898, 'min_sampl...               0.10   \n",
       "6   {'max_features': 0.3133829749087653, 'min_samp...               0.10   \n",
       "5   {'max_features': 0.25564467077774433, 'min_sam...               0.10   \n",
       "1   {'max_features': 0.3469737191337041, 'min_samp...               0.09   \n",
       "54  {'max_features': 0.29070217042516266, 'min_sam...               0.09   \n",
       "45  {'max_features': 0.3019429051381534, 'min_samp...               0.08   \n",
       "66  {'max_features': 0.26840050097398654, 'min_sam...               0.10   \n",
       "..                                                ...                ...   \n",
       "24  {'max_features': 0.2631924653799897, 'min_samp...               0.09   \n",
       "10  {'max_features': 0.31939667287499185, 'min_sam...               0.07   \n",
       "78  {'max_features': 0.2748597431680584, 'min_samp...               0.04   \n",
       "47  {'max_features': 0.2516391642123631, 'min_samp...               0.03   \n",
       "34  {'max_features': 0.33519224131235903, 'min_sam...               0.01   \n",
       "68  {'max_features': 0.26706585690456447, 'min_sam...              -0.00   \n",
       "93  {'max_features': 0.25851067852734533, 'min_sam...               0.00   \n",
       "21  {'max_features': 0.28539519937275837, 'min_sam...               0.01   \n",
       "40  {'max_features': 0.30381323520240094, 'min_sam...               0.01   \n",
       "26  {'max_features': 0.2893134141667649, 'min_samp...               0.04   \n",
       "37  {'max_features': 0.2847861387468348, 'min_samp...               0.06   \n",
       "0   {'max_features': 0.2565337691858373, 'min_samp...               0.05   \n",
       "97  {'max_features': 0.253018531021254, 'min_sampl...              -0.01   \n",
       "64  {'max_features': 0.33835455172992307, 'min_sam...              -0.05   \n",
       "61  {'max_features': 0.2509052550079607, 'min_samp...               0.02   \n",
       "33  {'max_features': 0.2594262824927381, 'min_samp...              -0.03   \n",
       "83  {'max_features': 0.265886711497229, 'min_sampl...              -0.04   \n",
       "50  {'max_features': 0.2645195547920704, 'min_samp...              -0.04   \n",
       "88  {'max_features': 0.2940329381111786, 'min_samp...              -0.02   \n",
       "91  {'max_features': 0.2723092305815467, 'min_samp...              -0.00   \n",
       "94  {'max_features': 0.2734346879311414, 'min_samp...              -0.07   \n",
       "95  {'max_features': 0.290211138184583, 'min_sampl...              -0.11   \n",
       "74  {'max_features': 0.29434756772212495, 'min_sam...              -0.05   \n",
       "42  {'max_features': 0.28419551357813755, 'min_sam...              -0.11   \n",
       "32  {'max_features': 0.29606047047625894, 'min_sam...              -0.07   \n",
       "65  {'max_features': 0.2603400004237625, 'min_samp...              -0.12   \n",
       "99  {'max_features': 0.2792492309654589, 'min_samp...              -0.11   \n",
       "25  {'max_features': 0.27025512862349726, 'min_sam...              -0.14   \n",
       "80  {'max_features': 0.3463257215073301, 'min_samp...              -0.11   \n",
       "60  {'max_features': 0.2533356223702762, 'min_samp...              -0.11   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "41               0.19               0.16               0.13   \n",
       "79               0.17               0.13               0.15   \n",
       "55               0.18               0.14               0.14   \n",
       "77               0.18               0.15               0.13   \n",
       "3                0.16               0.13               0.13   \n",
       "7                0.15               0.12               0.14   \n",
       "90               0.15               0.12               0.12   \n",
       "38               0.09               0.08               0.14   \n",
       "76               0.11               0.10               0.11   \n",
       "86               0.09               0.09               0.11   \n",
       "28               0.08               0.09               0.10   \n",
       "62               0.09               0.10               0.10   \n",
       "9                0.09               0.09               0.10   \n",
       "27               0.09               0.09               0.10   \n",
       "44               0.07               0.10               0.10   \n",
       "92               0.09               0.08               0.11   \n",
       "69               0.08               0.08               0.09   \n",
       "18               0.09               0.09               0.09   \n",
       "8                0.10               0.10               0.08   \n",
       "29               0.08               0.09               0.09   \n",
       "52               0.09               0.09               0.08   \n",
       "75               0.08               0.08               0.10   \n",
       "71               0.08               0.08               0.08   \n",
       "73               0.09               0.08               0.08   \n",
       "6                0.09               0.07               0.08   \n",
       "5                0.06               0.07               0.07   \n",
       "1                0.07               0.07               0.07   \n",
       "54               0.07               0.08               0.08   \n",
       "45               0.06               0.08               0.09   \n",
       "66               0.08               0.07               0.06   \n",
       "..                ...                ...                ...   \n",
       "24              -0.01               0.01              -0.01   \n",
       "10              -0.02              -0.01               0.02   \n",
       "78              -0.03              -0.02              -0.01   \n",
       "47              -0.03              -0.01              -0.00   \n",
       "34              -0.02              -0.01              -0.01   \n",
       "68              -0.02              -0.02              -0.00   \n",
       "93              -0.02              -0.02              -0.03   \n",
       "21              -0.03              -0.02              -0.02   \n",
       "40              -0.03              -0.03              -0.02   \n",
       "26              -0.03              -0.03              -0.02   \n",
       "37              -0.01              -0.01              -0.12   \n",
       "0               -0.05              -0.04              -0.05   \n",
       "97              -0.04              -0.05              -0.01   \n",
       "64              -0.03              -0.03              -0.02   \n",
       "61              -0.07              -0.04              -0.03   \n",
       "33              -0.04              -0.04              -0.02   \n",
       "83              -0.04              -0.04              -0.04   \n",
       "50              -0.04              -0.04              -0.04   \n",
       "88              -0.04              -0.08              -0.01   \n",
       "91              -0.08              -0.06              -0.04   \n",
       "94              -0.07              -0.06              -0.04   \n",
       "95              -0.10              -0.05              -0.04   \n",
       "74              -0.13              -0.08              -0.11   \n",
       "42              -0.10              -0.13              -0.08   \n",
       "32              -0.13              -0.09              -0.12   \n",
       "65              -0.14              -0.09              -0.13   \n",
       "99              -0.14              -0.08              -0.13   \n",
       "25              -0.13              -0.14              -0.07   \n",
       "80              -0.13              -0.12              -0.12   \n",
       "60              -0.13              -0.13              -0.12   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "41               0.17             0.16            0.02                1  \n",
       "79               0.17             0.16            0.02                2  \n",
       "55               0.17             0.15            0.02                3  \n",
       "77               0.15             0.15            0.02                4  \n",
       "3                0.16             0.14            0.01                5  \n",
       "7                0.15             0.14            0.01                6  \n",
       "90               0.12             0.13            0.01                7  \n",
       "38               0.15             0.12            0.03                8  \n",
       "76               0.11             0.11            0.00                9  \n",
       "86               0.11             0.10            0.01               10  \n",
       "28               0.12             0.10            0.01               11  \n",
       "62               0.11             0.10            0.01               12  \n",
       "9                0.11             0.10            0.01               13  \n",
       "27               0.10             0.10            0.01               14  \n",
       "44               0.11             0.09            0.01               15  \n",
       "92               0.08             0.09            0.01               16  \n",
       "69               0.11             0.09            0.01               17  \n",
       "18               0.09             0.09            0.01               18  \n",
       "8                0.08             0.09            0.01               19  \n",
       "29               0.10             0.09            0.01               20  \n",
       "52               0.09             0.09            0.01               21  \n",
       "75               0.09             0.09            0.01               22  \n",
       "71               0.09             0.09            0.01               23  \n",
       "73               0.09             0.09            0.01               24  \n",
       "6                0.08             0.08            0.01               25  \n",
       "5                0.10             0.08            0.01               26  \n",
       "1                0.09             0.08            0.01               27  \n",
       "54               0.07             0.08            0.01               28  \n",
       "45               0.09             0.08            0.01               29  \n",
       "66               0.07             0.08            0.01               30  \n",
       "..                ...              ...             ...              ...  \n",
       "24              -0.03             0.01            0.04               71  \n",
       "10              -0.01             0.01            0.03               72  \n",
       "78               0.02             0.00            0.03               73  \n",
       "47              -0.02            -0.01            0.02               74  \n",
       "34              -0.01            -0.01            0.01               75  \n",
       "68              -0.02            -0.01            0.01               76  \n",
       "93              -0.00            -0.01            0.01               77  \n",
       "21              -0.01            -0.01            0.01               78  \n",
       "40              -0.02            -0.02            0.01               79  \n",
       "26              -0.05            -0.02            0.03               80  \n",
       "37              -0.03            -0.02            0.06               81  \n",
       "0               -0.04            -0.03            0.04               82  \n",
       "97              -0.03            -0.03            0.01               83  \n",
       "64              -0.02            -0.03            0.01               84  \n",
       "61              -0.05            -0.03            0.03               85  \n",
       "33              -0.05            -0.04            0.01               86  \n",
       "83              -0.03            -0.04            0.00               87  \n",
       "50              -0.04            -0.04            0.00               88  \n",
       "88              -0.07            -0.04            0.03               89  \n",
       "91              -0.04            -0.04            0.02               90  \n",
       "94              -0.04            -0.06            0.01               91  \n",
       "95              -0.04            -0.07            0.03               92  \n",
       "74              -0.08            -0.09            0.03               93  \n",
       "42              -0.09            -0.10            0.02               94  \n",
       "32              -0.12            -0.10            0.02               95  \n",
       "65              -0.07            -0.11            0.03               96  \n",
       "99              -0.13            -0.12            0.02               97  \n",
       "25              -0.13            -0.12            0.03               98  \n",
       "80              -0.12            -0.12            0.01               99  \n",
       "60              -0.13            -0.12            0.01              100  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5.1.Logistic Regression\n",
    "- C : Regularization value, the more, the stronger the regularization(double). \n",
    "- RegularizationType: Can be either \"L2\" or “L1”. Default is “L2”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LR_param_grid = {\n",
    "    # randomly sample numbers from 4 to 204 estimators\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    # normally distributed max_featur,es, with mean .25 stddev 0.1, bounded between 0 and 1\n",
    "    'C' : uniform(0,4)\n",
    "    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n",
    "    'min_samples_split': uniform(0.01, 0.199) # 0 to 20% obs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression()\n",
    "\n",
    "clf = RandomizedSearchCV(LR_model, LR_param_grid, n_iter=100, cv=5, random_state=SEED, score = score)\n",
    "model = clf.fit(x_train, y_train)\n",
    "lr_best = model.best_estimator_.get_params()\n",
    "print(lr_best)\n",
    "cv_results = model.cv_results_\n",
    "scores = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5.6 GradientBoosting - if you have time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "# gbm_param_grid = {\n",
    "#     'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "#     'num_leaves': list(range(20, 150)),\n",
    "#     'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "#     'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "#     'min_child_samples': list(range(20, 500, 5)),\n",
    "#     'reg_alpha': list(np.linspace(0, 1)),\n",
    "#     'reg_lambda': list(np.linspace(0, 1)),\n",
    "#     'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "#     'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "#     'is_unbalance': [True, False]\n",
    "# }\n",
    "\n",
    "gbm_param_grid = {\n",
    "    'learning_rate_value' = uniform(0,1),\n",
    "    'n_estimators_value' = randint(50,300)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "clf = RandomizedSearchCV(gb_model, gb_param_grid, n_iter=100, cv=5, random_state=SEED, score = score)\n",
    "model = clf.fit(x_train, y_train)\n",
    "gb_best = model.best_estimator_.get_params()\n",
    "print(gb_best)\n",
    "cv_results = model.cv_results_\n",
    "scores = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5.2. KNN\n",
    "- n_neighbors: Number of neighbors to use by default for k_neighbors queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "'neighbors' : randint(1,20)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "KN_model = KNeighborsClassifier()()\n",
    "\n",
    "clf = RandomizedSearchCV(KN_model, knn_param_grid, n_iter=100, cv=5, random_state=SEED, score = score)\n",
    "model = clf.fit(x_train, y_train)\n",
    "kn_best = model.best_estimator_.get_params()\n",
    "print(kn_best)\n",
    "cv_results = model.cv_results_\n",
    "scores = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:10:46.975348Z",
     "start_time": "2020-03-04T17:10:46.962945Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## 5.3. SVC \n",
    "- C: The Penalty parameter C of the error term. \n",
    "- Kernel: Kernel type could be linear, poly, rbf or sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svc_param_grid = {\n",
    "    'c_values' : uniform(0,3),\n",
    "    'kernel_values' : [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svc_model = SVC()()\n",
    "\n",
    "clf = RandomizedSearchCV(svc_model, svc_param_grid, n_iter=100, cv=5, random_state=SEED, score = score)\n",
    "model = clf.fit(x_train, y_train)\n",
    "svc_best = model.best_estimator_.get_params()\n",
    "print(svc_best)\n",
    "cv_results = model.cv_results_\n",
    "scores = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T17:54:14.211698Z",
     "start_time": "2020-03-04T17:54:14.186735Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-cc8c822833ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrv_discrete\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrv_discrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'truncnorm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNh\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mNh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, a, b, name, badvalue, moment_tol, values, inc, longname, shapes, extradoc, seed)\u001b[0m\n\u001b[0;32m   3359\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvecentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m         \u001b[0mxk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rv_discrete\n",
    "rv_discrete(name='truncnorm', values=(Nh/4, 5*Nh, Nh, Nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:17:27.518782Z",
     "start_time": "2020-03-04T20:17:27.511801Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Binomial(n, p) ~ Normal(n*p, sqrt(n*p*(1-p)))\n",
    "# bi = np.random.binomial(n=100, p=0.5, size=10000)\n",
    "\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': randint(Nh/4,5*Nh),\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': uniform(1e-4,1),\n",
    "    'learning_rate': ['constant','adaptive']\n",
    "    ,'learning_rate_init': uniform(1e-3,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:15:42.039931Z",
     "start_time": "2020-03-04T20:15:42.026260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:30:39.412231Z",
     "start_time": "2020-03-04T20:17:29.607027Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': 1181, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 100, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'socres' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-185-5683c1f3f8f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rank_test_score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msocres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'socres' is not defined"
     ]
    }
   ],
   "source": [
    "mlp_model = MLPRegressor(max_iter=100)\n",
    "\n",
    "clf = RandomizedSearchCV(mlp_model, mlp_param_grid, n_iter=10, cv=5, random_state=SEED, scoring = score, verbose = 1)\n",
    "model = clf.fit(x_train, y_train)\n",
    "mlp_best = model.best_estimator_.get_params()\n",
    "print(mlp_best)\n",
    "cv_results = model.cv_results_\n",
    "scores = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:15:29.506242Z",
     "start_time": "2020-03-04T20:15:29.500217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:15:22.390370Z",
     "start_time": "2020-03-04T20:15:22.382391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isinf(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:32:22.259769Z",
     "start_time": "2020-03-04T20:31:05.182244Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\didio\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=1181, learning_rate='adaptive',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=300,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_best['max_iter'] = 300\n",
    "mlp = MLPRegressor(**mlp_best)\n",
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T20:45:15.889975Z",
     "start_time": "2020-03-04T20:45:14.502758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000002488C597550>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000002488B9EF470>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = mlp.predict(x_test)\n",
    "pd.DataFrame([y_pred,y_train],['Predicted','Test']).T.hist( alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repport\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "confusion_matrix(y_true, y_pred ) #, labels=[\"ant\", \"bird\", \"cat\"])\n",
    "\n",
    "print(classification_report(y_test, y_pred ) )  #,target_names=target_names))\n",
    "\n",
    "\n",
    "# explained_variance_score(y_test, y_pred)\n",
    "# metrics.r2_score(y_test, y_pred)\n",
    "# metrics.mean_squared_error(y_test, y_pred, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([y_pred,y_train],['Predicted','Test']).T.hist( alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTIIING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "model1 = LogisticRegression(**lr_best)\n",
    "\n",
    "model8 = RandomForestClassifier(**rf_best)\n",
    "\n",
    "model2 = KNeighborsClassifier(**kn_best)\n",
    "\n",
    "model3 = SVC(**svc_best)\n",
    "\n",
    "model6 = GradientBoostingClassifier(**gb_best)\n",
    "\n",
    "model10 = MLPClassifier(**mlp_best)\n",
    "# max_depth_value = [3, None]\n",
    "# max_features_value =  randint(1, 4)\n",
    "# min_samples_leaf_value = randint(1, 4)\n",
    "# criterion_value = [\"gini\", \"entropy\"]\n",
    "# model4 = DecisionTreeClassifier(**param)\n",
    "\n",
    "# learning_rate_value = [.01,.05,.1,.5,1]\n",
    "# n_estimators_value = [50,100,150,200,250,300]\n",
    "# model5 = AdaBoostClassifier(**param)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sub models\n",
    "estimators = [('LR',model1), ('KNN',model2), ('SVC',model3),\n",
    "             # ('DT',model4),\n",
    "             # ('ADa',model5),\n",
    "              ('GB',model6),\n",
    "              ('NB',model7), ('RF',model8) \n",
    "              #,('ET',model9)\n",
    "              ,('MLP',model10)\n",
    "             ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ensemble model\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=SEED)\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "results = cross_val_score(ensemble, x_train,y_train, cv=kfold)\n",
    "\n",
    "print('Accuracy on train: ',results.mean())\n",
    "ensemble_model = ensemble.fit(x_train,y_train)\n",
    "\n",
    "pred = ensemble_model.predict(X_test)\n",
    "print('Accuracy on test:' , (y_test == pred).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "6",
    "lenType": "6",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
